[
  {
    "objectID": "Homework9.html",
    "href": "Homework9.html",
    "title": "Homework9",
    "section": "",
    "text": "Throughout, we will use abbreviations for mean absolute error (MAE) and root mean square error (RMSE)."
  },
  {
    "objectID": "Homework9.html#note-on-abbreviations",
    "href": "Homework9.html#note-on-abbreviations",
    "title": "Homework9",
    "section": "",
    "text": "Throughout, we will use abbreviations for mean absolute error (MAE) and root mean square error (RMSE)."
  },
  {
    "objectID": "Homework9.html#read-in-the-data",
    "href": "Homework9.html#read-in-the-data",
    "title": "Homework9",
    "section": "Read in the Data",
    "text": "Read in the Data\n\nlibrary(baguette)\n\nLoading required package: parsnip\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\nlibrary(ggplot2)\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(ranger)\n#library(randomForest)\nlibrary(stringr)\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tibble       3.2.1\n✔ dplyr        1.1.4     ✔ tidyr        1.3.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ purrr        1.0.2     ✔ workflowsets 1.1.0\n✔ recipes      1.1.0     ✔ yardstick    1.3.1\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard()  masks scales::discard()\n✖ tidyr::expand()   masks Matrix::expand()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ tidyr::pack()     masks Matrix::pack()\n✖ recipes::step()   masks stats::step()\n✖ tidyr::unpack()   masks Matrix::unpack()\n✖ recipes::update() masks Matrix::update(), stats::update()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats 1.0.0     ✔ readr   2.1.5\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ tidyr::expand()     masks Matrix::expand()\n✖ dplyr::filter()     masks stats::filter()\n✖ recipes::fixed()    masks stringr::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ tidyr::pack()       masks Matrix::pack()\n✖ readr::spec()       masks yardstick::spec()\n✖ tidyr::unpack()     masks Matrix::unpack()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\n#Read in the data, using a different hint from the assignment suggestion just to have a variety, for learning!\n#seoul_bike_data &lt;- read.csv(\"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\",fileEncoding='latin1',check.names=F)\n\nseoul_bike_data &lt;-\n  read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\",locale=locale(encoding=\"latin1\"))\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Homework9.html#eda",
    "href": "Homework9.html#eda",
    "title": "Homework9",
    "section": "EDA",
    "text": "EDA\nCheck for missing data:\n\n#We are told that there is no missing data, but we can check for missing data\nseoul_bike_data |&gt;\n  is.na() |&gt;\n  colSums()\n\n                     Date         Rented Bike Count                      Hour \n                        0                         0                         0 \n          Temperature(°C)               Humidity(%)          Wind speed (m/s) \n                        0                         0                         0 \n         Visibility (10m) Dew point temperature(°C)   Solar Radiation (MJ/m2) \n                        0                         0                         0 \n             Rainfall(mm)             Snowfall (cm)                   Seasons \n                        0                         0                         0 \n                  Holiday           Functioning Day \n                        0                         0 \n\n\nAll variables are 0, so no missing data!\nNext, check the column types and the values within the columns to make sure they make sense (basic summary stats for numeric columns and check the unique values for the categorical variables).\n\n#Column types\nattributes(seoul_bike_data)$spec\n\ncols(\n  Date = col_character(),\n  `Rented Bike Count` = col_double(),\n  Hour = col_double(),\n  `Temperature(°C)` = col_double(),\n  `Humidity(%)` = col_double(),\n  `Wind speed (m/s)` = col_double(),\n  `Visibility (10m)` = col_double(),\n  `Dew point temperature(°C)` = col_double(),\n  `Solar Radiation (MJ/m2)` = col_double(),\n  `Rainfall(mm)` = col_double(),\n  `Snowfall (cm)` = col_double(),\n  Seasons = col_character(),\n  Holiday = col_character(),\n  `Functioning Day` = col_character()\n)\n\n\nWe have numeric (double) columns: Rented Bike Count, Hour, Humidity, and Visibility. We have numeric columns: Temperature, Wind speed, Solar Radiation, Rainfall, and Snowfall. We have character columns: Date, Seasons, Holiday, and Functioning Day. However, we want Date to be in a date format:\n\nseoul_bike_data &lt;- seoul_bike_data |&gt;\n  mutate(Date = dmy(Date))\n\n\nSummary Statistics\nSummarize the columns:\n\nsummary(seoul_bike_data)\n\n      Date            Rented Bike Count      Hour       Temperature(°C) \n Min.   :2017-12-01   Min.   :   0.0    Min.   : 0.00   Min.   :-17.80  \n 1st Qu.:2018-03-02   1st Qu.: 191.0    1st Qu.: 5.75   1st Qu.:  3.50  \n Median :2018-06-01   Median : 504.5    Median :11.50   Median : 13.70  \n Mean   :2018-06-01   Mean   : 704.6    Mean   :11.50   Mean   : 12.88  \n 3rd Qu.:2018-08-31   3rd Qu.:1065.2    3rd Qu.:17.25   3rd Qu.: 22.50  \n Max.   :2018-11-30   Max.   :3556.0    Max.   :23.00   Max.   : 39.40  \n  Humidity(%)    Wind speed (m/s) Visibility (10m) Dew point temperature(°C)\n Min.   : 0.00   Min.   :0.000    Min.   :  27     Min.   :-30.600          \n 1st Qu.:42.00   1st Qu.:0.900    1st Qu.: 940     1st Qu.: -4.700          \n Median :57.00   Median :1.500    Median :1698     Median :  5.100          \n Mean   :58.23   Mean   :1.725    Mean   :1437     Mean   :  4.074          \n 3rd Qu.:74.00   3rd Qu.:2.300    3rd Qu.:2000     3rd Qu.: 14.800          \n Max.   :98.00   Max.   :7.400    Max.   :2000     Max.   : 27.200          \n Solar Radiation (MJ/m2)  Rainfall(mm)     Snowfall (cm)       Seasons         \n Min.   :0.0000          Min.   : 0.0000   Min.   :0.00000   Length:8760       \n 1st Qu.:0.0000          1st Qu.: 0.0000   1st Qu.:0.00000   Class :character  \n Median :0.0100          Median : 0.0000   Median :0.00000   Mode  :character  \n Mean   :0.5691          Mean   : 0.1487   Mean   :0.07507                     \n 3rd Qu.:0.9300          3rd Qu.: 0.0000   3rd Qu.:0.00000                     \n Max.   :3.5200          Max.   :35.0000   Max.   :8.80000                     \n   Holiday          Functioning Day   \n Length:8760        Length:8760       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n\n\n\nCheck Unique Values for Character Columns\n\n#Bike data character summary\nbike_data_chars &lt;- seoul_bike_data |&gt;\n  select(where(is.character))\nmap(bike_data_chars, ~str_c(unique(.x),collapse = \", \")) |&gt;\n  bind_rows() |&gt;\n  gather(key = col_name, value = col_unique)\n\n# A tibble: 3 × 2\n  col_name        col_unique                    \n  &lt;chr&gt;           &lt;chr&gt;                         \n1 Seasons         Winter, Spring, Summer, Autumn\n2 Holiday         No Holiday, Holiday           \n3 Functioning Day Yes, No                       \n\nseoul_bike_data\n\n# A tibble: 8,760 × 14\n   Date       `Rented Bike Count`  Hour `Temperature(°C)` `Humidity(%)`\n   &lt;date&gt;                   &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;\n 1 2017-12-01                 254     0              -5.2            37\n 2 2017-12-01                 204     1              -5.5            38\n 3 2017-12-01                 173     2              -6              39\n 4 2017-12-01                 107     3              -6.2            40\n 5 2017-12-01                  78     4              -6              36\n 6 2017-12-01                 100     5              -6.4            37\n 7 2017-12-01                 181     6              -6.6            35\n 8 2017-12-01                 460     7              -7.4            38\n 9 2017-12-01                 930     8              -7.6            37\n10 2017-12-01                 490     9              -6.5            27\n# ℹ 8,750 more rows\n# ℹ 9 more variables: `Wind speed (m/s)` &lt;dbl&gt;, `Visibility (10m)` &lt;dbl&gt;,\n#   `Dew point temperature(°C)` &lt;dbl&gt;, `Solar Radiation (MJ/m2)` &lt;dbl&gt;,\n#   `Rainfall(mm)` &lt;dbl&gt;, `Snowfall (cm)` &lt;dbl&gt;, Seasons &lt;chr&gt;, Holiday &lt;chr&gt;,\n#   `Functioning Day` &lt;chr&gt;\n\n\nSince the Functioning Day variable is sometimes “No,” we can restrict to those days when it is “Yes.”\n\nseoul_bike_data &lt;- seoul_bike_data %&gt;%\n  filter(`Functioning Day` == \"Yes\")\n\nseoul_bike_data\n\n# A tibble: 8,465 × 14\n   Date       `Rented Bike Count`  Hour `Temperature(°C)` `Humidity(%)`\n   &lt;date&gt;                   &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;\n 1 2017-12-01                 254     0              -5.2            37\n 2 2017-12-01                 204     1              -5.5            38\n 3 2017-12-01                 173     2              -6              39\n 4 2017-12-01                 107     3              -6.2            40\n 5 2017-12-01                  78     4              -6              36\n 6 2017-12-01                 100     5              -6.4            37\n 7 2017-12-01                 181     6              -6.6            35\n 8 2017-12-01                 460     7              -7.4            38\n 9 2017-12-01                 930     8              -7.6            37\n10 2017-12-01                 490     9              -6.5            27\n# ℹ 8,455 more rows\n# ℹ 9 more variables: `Wind speed (m/s)` &lt;dbl&gt;, `Visibility (10m)` &lt;dbl&gt;,\n#   `Dew point temperature(°C)` &lt;dbl&gt;, `Solar Radiation (MJ/m2)` &lt;dbl&gt;,\n#   `Rainfall(mm)` &lt;dbl&gt;, `Snowfall (cm)` &lt;dbl&gt;, Seasons &lt;chr&gt;, Holiday &lt;chr&gt;,\n#   `Functioning Day` &lt;chr&gt;\n\n\n\n\nTurn the character variables (Seasons, Holiday, and Functioning Day) into factors.\n\nseoul_bike_data &lt;- seoul_bike_data |&gt;\n  mutate(across(c(Seasons,Holiday,'Functioning Day'),factor))\n\nseoul_bike_data\n\n# A tibble: 8,465 × 14\n   Date       `Rented Bike Count`  Hour `Temperature(°C)` `Humidity(%)`\n   &lt;date&gt;                   &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;\n 1 2017-12-01                 254     0              -5.2            37\n 2 2017-12-01                 204     1              -5.5            38\n 3 2017-12-01                 173     2              -6              39\n 4 2017-12-01                 107     3              -6.2            40\n 5 2017-12-01                  78     4              -6              36\n 6 2017-12-01                 100     5              -6.4            37\n 7 2017-12-01                 181     6              -6.6            35\n 8 2017-12-01                 460     7              -7.4            38\n 9 2017-12-01                 930     8              -7.6            37\n10 2017-12-01                 490     9              -6.5            27\n# ℹ 8,455 more rows\n# ℹ 9 more variables: `Wind speed (m/s)` &lt;dbl&gt;, `Visibility (10m)` &lt;dbl&gt;,\n#   `Dew point temperature(°C)` &lt;dbl&gt;, `Solar Radiation (MJ/m2)` &lt;dbl&gt;,\n#   `Rainfall(mm)` &lt;dbl&gt;, `Snowfall (cm)` &lt;dbl&gt;, Seasons &lt;fct&gt;, Holiday &lt;fct&gt;,\n#   `Functioning Day` &lt;fct&gt;\n\n\n\n\nRename all the variables to have easy-to-use names\n\ncolnames(seoul_bike_data) &lt;- gsub(\"\\\\s*\\\\([^\\\\)]+\\\\)\", \"\", colnames(seoul_bike_data))\ncolnames(seoul_bike_data) &lt;- str_trim(colnames(seoul_bike_data), \"right\")\ncolnames(seoul_bike_data) &lt;- gsub(\" \",\"_\",colnames(seoul_bike_data))\n\nseoul_bike_data\n\n# A tibble: 8,465 × 14\n   Date       Rented_Bike_Count  Hour Temperature Humidity Wind_speed Visibility\n   &lt;date&gt;                 &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 2017-12-01               254     0        -5.2       37        2.2       2000\n 2 2017-12-01               204     1        -5.5       38        0.8       2000\n 3 2017-12-01               173     2        -6         39        1         2000\n 4 2017-12-01               107     3        -6.2       40        0.9       2000\n 5 2017-12-01                78     4        -6         36        2.3       2000\n 6 2017-12-01               100     5        -6.4       37        1.5       2000\n 7 2017-12-01               181     6        -6.6       35        1.3       2000\n 8 2017-12-01               460     7        -7.4       38        0.9       2000\n 9 2017-12-01               930     8        -7.6       37        1.1       2000\n10 2017-12-01               490     9        -6.5       27        0.5       1928\n# ℹ 8,455 more rows\n# ℹ 7 more variables: Dew_point_temperature &lt;dbl&gt;, Solar_Radiation &lt;dbl&gt;,\n#   Rainfall &lt;dbl&gt;, Snowfall &lt;dbl&gt;, Seasons &lt;fct&gt;, Holiday &lt;fct&gt;,\n#   Functioning_Day &lt;fct&gt;\n\n\n\n\nCreate summary statistics (especially related to the bike rental count). These should be done across the categorical variables as well.\n\nseoul_bike_data &lt;- seoul_bike_data |&gt;\n  group_by(Date, Seasons, Holiday) |&gt;\n  summarize(Rented_Bike_Count = sum(Rented_Bike_Count),\n            Temperature = mean(Temperature),\n            Humidity = mean(Humidity),\n            Wind_speed = mean(Wind_speed),\n            Visibility = mean(Visibility),\n            Dew_point_temperature = mean(Dew_point_temperature),\n            Solar_Radiation = mean(Solar_Radiation),\n            Rainfall = sum(Rainfall),\n            Snowfall = sum(Snowfall)) |&gt;\n  ungroup()\n\n`summarise()` has grouped output by 'Date', 'Seasons'. You can override using\nthe `.groups` argument.\n\nseoul_bike_data\n\n# A tibble: 353 × 12\n   Date       Seasons Holiday  Rented_Bike_Count Temperature Humidity Wind_speed\n   &lt;date&gt;     &lt;fct&gt;   &lt;fct&gt;                &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1 2017-12-01 Winter  No Holi…              9539     -2.45       45.9      1.54 \n 2 2017-12-02 Winter  No Holi…              8523      1.33       62.0      1.71 \n 3 2017-12-03 Winter  No Holi…              7222      4.88       81.5      1.61 \n 4 2017-12-04 Winter  No Holi…              8729     -0.304      52.5      3.45 \n 5 2017-12-05 Winter  No Holi…              8307     -4.46       36.4      1.11 \n 6 2017-12-06 Winter  No Holi…              6669      0.0458     70.8      0.696\n 7 2017-12-07 Winter  No Holi…              8549      1.09       67.5      1.69 \n 8 2017-12-08 Winter  No Holi…              8032     -3.82       41.8      1.85 \n 9 2017-12-09 Winter  No Holi…              7233     -0.846      46        1.08 \n10 2017-12-10 Winter  No Holi…              3453      1.19       69.7      2.00 \n# ℹ 343 more rows\n# ℹ 5 more variables: Visibility &lt;dbl&gt;, Dew_point_temperature &lt;dbl&gt;,\n#   Solar_Radiation &lt;dbl&gt;, Rainfall &lt;dbl&gt;, Snowfall &lt;dbl&gt;\n\n\n\n\nRecreate your basic summary stats and then create some plots to explore relationships. Report correlation between your numeric variables as well.\n\nseoul_bike_data |&gt;\nsummarize(across(`Rented_Bike_Count`,\n.fns = c(\"mean\" = mean,\n\"median\" = median,\n\"sd\" = sd,\n\"IQR\" = IQR,\n\"min\" = min,\n\"max\" = max),\n.names = \"{.col}_{.fn}\"))\n\n# A tibble: 1 × 6\n  Rented_Bike_Count_mean Rented_Bike_Count_median Rented_Bike_Count_sd\n                   &lt;dbl&gt;                    &lt;dbl&gt;                &lt;dbl&gt;\n1                 17485.                    18563                9937.\n# ℹ 3 more variables: Rented_Bike_Count_IQR &lt;dbl&gt;, Rented_Bike_Count_min &lt;dbl&gt;,\n#   Rented_Bike_Count_max &lt;dbl&gt;\n\n\n\nseoul_bike_data |&gt;\ngroup_by(Holiday) |&gt;\nsummarize(across(`Rented_Bike_Count`,\n.fns = c(\"mean\" = mean,\n\"median\" = median,\n\"sd\" = sd,\n\"IQR\" = IQR,\n\"min\" = min,\n\"max\" = max),\n.names = \"{.col}_{.fn}\"))\n\n# A tibble: 2 × 7\n  Holiday    Rented_Bike_Count_mean Rented_Bike_Count_med…¹ Rented_Bike_Count_sd\n  &lt;fct&gt;                       &lt;dbl&gt;                   &lt;dbl&gt;                &lt;dbl&gt;\n1 Holiday                    12700.                   7184                10504.\n2 No Holiday                 17727.                  19104.                9862.\n# ℹ abbreviated name: ¹​Rented_Bike_Count_median\n# ℹ 3 more variables: Rented_Bike_Count_IQR &lt;dbl&gt;, Rented_Bike_Count_min &lt;dbl&gt;,\n#   Rented_Bike_Count_max &lt;dbl&gt;\n\n\n\nseoul_bike_data |&gt;\ngroup_by(Seasons) |&gt;\nsummarize(across(`Rented_Bike_Count`,\n.fns = c(\"mean\" = mean,\n\"median\" = median,\n\"sd\" = sd,\n\"IQR\" = IQR,\n\"min\" = min,\n\"max\" = max),\n.names = \"{.col}_{.fn}\"))\n\n# A tibble: 4 × 7\n  Seasons Rented_Bike_Count_mean Rented_Bike_Count_median Rented_Bike_Count_sd\n  &lt;fct&gt;                    &lt;dbl&gt;                    &lt;dbl&gt;                &lt;dbl&gt;\n1 Autumn                  22099.                   23350                 6711.\n2 Spring                  17910.                   17590                 8357.\n3 Summer                  24818.                   25572.                7297.\n4 Winter                   5413.                    5498                 1808.\n# ℹ 3 more variables: Rented_Bike_Count_IQR &lt;dbl&gt;, Rented_Bike_Count_min &lt;dbl&gt;,\n#   Rented_Bike_Count_max &lt;dbl&gt;\n\n\n\nseoul_bike_data |&gt;\ngroup_by(Seasons, Holiday) |&gt;\nsummarize(across(`Rented_Bike_Count`,\n.fns = c(\"mean\" = mean,\n\"median\" = median,\n\"sd\" = sd,\n\"IQR\" = IQR,\n\"min\" = min,\n\"max\" = max),\n.names = \"{.col}_{.fn}\"))\n\n`summarise()` has grouped output by 'Seasons'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 8\n# Groups:   Seasons [4]\n  Seasons Holiday    Rented_Bike_Count_mean Rented_Bike_Count_median\n  &lt;fct&gt;   &lt;fct&gt;                       &lt;dbl&gt;                    &lt;dbl&gt;\n1 Autumn  Holiday                    22754.                   21705 \n2 Autumn  No Holiday                 22065.                   23472 \n3 Spring  Holiday                    15247.                   13790 \n4 Spring  No Holiday                 18002.                   17730 \n5 Summer  Holiday                    24532.                   24532.\n6 Summer  No Holiday                 24824.                   25572.\n7 Winter  Holiday                     3759                     3454.\n8 Winter  No Holiday                  5574.                    5609 \n# ℹ 4 more variables: Rented_Bike_Count_sd &lt;dbl&gt;, Rented_Bike_Count_IQR &lt;dbl&gt;,\n#   Rented_Bike_Count_min &lt;dbl&gt;, Rented_Bike_Count_max &lt;dbl&gt;\n\n\nThe correlation is as follows:\n\n#Table display\nM &lt;- seoul_bike_data |&gt;\n  select(where(is.numeric)) |&gt;\n  cor()\n\nM\n\n                      Rented_Bike_Count  Temperature    Humidity  Wind_speed\nRented_Bike_Count            1.00000000  0.753076732  0.03588697 -0.19288142\nTemperature                  0.75307673  1.000000000  0.40416749 -0.26072179\nHumidity                     0.03588697  0.404167486  1.00000000 -0.23425778\nWind_speed                  -0.19288142 -0.260721792 -0.23425778  1.00000000\nVisibility                   0.16599375  0.002336683 -0.55917733  0.20602264\nDew_point_temperature        0.65047655  0.962796255  0.63204729 -0.28770322\nSolar_Radiation              0.73589290  0.550274301 -0.27444967  0.09612635\nRainfall                    -0.23910905  0.144517274  0.52864263 -0.10167578\nSnowfall                    -0.26529110 -0.266963662  0.06539191  0.02088156\n                        Visibility Dew_point_temperature Solar_Radiation\nRented_Bike_Count      0.165993749             0.6504765      0.73589290\nTemperature            0.002336683             0.9627963      0.55027430\nHumidity              -0.559177334             0.6320473     -0.27444967\nWind_speed             0.206022636            -0.2877032      0.09612635\nVisibility             1.000000000            -0.1535516      0.27139591\nDew_point_temperature -0.153551591             1.0000000      0.38315713\nSolar_Radiation        0.271395906             0.3831571      1.00000000\nRainfall              -0.221993866             0.2645662     -0.32270413\nSnowfall              -0.101889019            -0.2095529     -0.23343056\n                         Rainfall    Snowfall\nRented_Bike_Count     -0.23910905 -0.26529110\nTemperature            0.14451727 -0.26696366\nHumidity               0.52864263  0.06539191\nWind_speed            -0.10167578  0.02088156\nVisibility            -0.22199387 -0.10188902\nDew_point_temperature  0.26456621 -0.20955286\nSolar_Radiation       -0.32270413 -0.23343056\nRainfall               1.00000000 -0.02313404\nSnowfall              -0.02313404  1.00000000\n\n#M &lt;- cor(seoul_bike_data |&gt; \n#  select(\n#      where(is.numeric)\n#    )\n#  )\n\n#Plot\ncorrplot(M, type = 'lower', method = 'number', diag = FALSE,col=colorRampPalette(c(\"green\",\"blue\"))(100),cl.ratio=0.07,tl.col = \"black\", tl.cex = 0.8, tl.srt = 45, mar = c(0,0,0,0), number.digits=3,number.cex=0.8) # colorful number"
  },
  {
    "objectID": "Homework9.html#split-the-data",
    "href": "Homework9.html#split-the-data",
    "title": "Homework9",
    "section": "Split the Data",
    "text": "Split the Data\nUse functions from tidymodels to split the data into a training and test set (75/25 split). Then, use the strata argument to stratify the split on the Seasons variable.\n\nset.seed(11)\nbike_split &lt;- initial_split(seoul_bike_data, prop = 0.75, strata = Seasons)\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)\nbike_10_fold &lt;- vfold_cv(bike_train, 10)\n\nOn the training set, create a 10–fold cross validation (CV) split\n\nbike_10_fold &lt;- vfold_cv(bike_train, 10)"
  },
  {
    "objectID": "Homework9.html#fitting-multiple-linear-regression-mlr-models",
    "href": "Homework9.html#fitting-multiple-linear-regression-mlr-models",
    "title": "Homework9",
    "section": "Fitting Multiple Linear Regression (MLR) Models",
    "text": "Fitting Multiple Linear Regression (MLR) Models\n\nRecipe 1\nUse the date variable to create a weekday/weekend factor variable. Then, standardize the numeric variables since their scales are pretty different. Finally, create dummy variables for the seasons, holiday, and new day type variable.\n\nbike_recipe1 &lt;- recipe(Rented_Bike_Count ~ ., data = bike_train) |&gt;\n  step_date(Date, features = \"dow\") |&gt;\n  step_mutate(WW = factor(ifelse(Date_dow %in% c(\"Sat\",\"Sun\"),\"weekend\",\"weekday\"))) |&gt; #ww=weekend/weekday\n  step_rm(Date, Date_dow) |&gt;\n  step_dummy(Seasons, Holiday, WW) |&gt;\n  step_normalize(all_numeric(),-Rented_Bike_Count) #|&gt;\n#  prep(training = bike_train) |&gt;\n#  bake(bike_train)\n\n#bike_recipe1\n\n\n\nRecipe 2\n\nbike_recipe2 &lt;- recipe(Rented_Bike_Count ~ ., data = bike_train) |&gt;\n  step_date(Date, features = \"dow\") |&gt;\n  step_mutate(WW = factor(ifelse(Date_dow %in% c(\"Sat\",\"Sun\"),\"weekend\",\"weekday\"))) |&gt;\n  step_rm(Date, Date_dow) |&gt;\n  step_dummy(Seasons, Holiday, WW) |&gt;\n  step_normalize(all_numeric(),-all_outcomes()) |&gt;\n  step_interact(terms=\n                  ~starts_with(\"Seasons\")*starts_with(\"Holiday\") +\n                  starts_with(\"Seasons\")*Temperature + \n                  Temperature*Rainfall)\n\n\n\nRecipe 3\n\nbike_recipe3 &lt;- recipe(Rented_Bike_Count ~ ., data = bike_train) |&gt;\n  step_date(Date, features = \"dow\") |&gt;\n  step_mutate(ww = factor(ifelse(Date_dow %in% c(\"Sat\",\"Sun\"),\"weekend\",\"weekday\"))) |&gt;\n  step_rm(Date, Date_dow) |&gt;\n  step_dummy(Seasons, Holiday, ww) |&gt;\n  step_normalize(all_numeric(),-all_outcomes()) |&gt;\n  step_interact(terms=\n                  ~starts_with(\"Seasons\")*starts_with(\"Holiday\") +\n                  starts_with(\"Seasons\")*Temperature + \n                  Temperature*Rainfall) |&gt;\n  step_poly(Temperature,\n            Wind_speed,\n            Visibility,\n            Dew_point_temperature,\n            Solar_Radiation,\n            Rainfall,\n            Snowfall,\n            degree = 2)\n\n\n\nSet up the linear model fit\n\nMLR_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\nCreate workflows, then fit the models using 10–fold CV. Then, consider the training set CV error to choose a best model.\n\nMLR_wkf1 &lt;- workflow() |&gt;\n  add_recipe(bike_recipe1) |&gt;\n  add_model(MLR_spec)\n\nMLR_CV_fit1 &lt;- MLR_wkf1 |&gt;\n  fit_resamples(bike_10_fold)\n\nMLR_wkf2 &lt;- workflow() |&gt;\n  add_recipe(bike_recipe2) |&gt;\n  add_model(MLR_spec)\n\nMLR_CV_fit2 &lt;- MLR_wkf2 |&gt;\n  fit_resamples(bike_10_fold)\n\nMLR_wkf3 &lt;- workflow() |&gt;\n  add_recipe(bike_recipe3) |&gt;\n  add_model(MLR_spec)\n\nMLR_CV_fit3 &lt;- MLR_wkf3 |&gt;\n  fit_resamples(bike_10_fold)\n\nCollect the metrics:\n\nrbind(MLR_CV_fit1 |&gt; collect_metrics(),\n      MLR_CV_fit2 |&gt; collect_metrics(),\n      MLR_CV_fit3 |&gt; collect_metrics())\n\n# A tibble: 6 × 6\n  .metric .estimator     mean     n  std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4194.       10 164.     Preprocessor1_Model1\n2 rsq     standard      0.821    10   0.0145 Preprocessor1_Model1\n3 rmse    standard   3075.       10 286.     Preprocessor1_Model1\n4 rsq     standard      0.905    10   0.0114 Preprocessor1_Model1\n5 rmse    standard   3043.       10 288.     Preprocessor1_Model1\n6 rsq     standard      0.907    10   0.0115 Preprocessor1_Model1\n\n\nThe last model has the smallest mean RMSE, so it looks to be the best model. Fit it to the entire training set, then see how it performs on the test set.\n\nfinal_fit &lt;- workflow() |&gt; \n  add_recipe(bike_recipe3) |&gt;\n  add_model(MLR_spec) |&gt;\n  last_fit(bike_split)\n\nfinal_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3285.    Preprocessor1_Model1\n2 rsq     standard       0.895 Preprocessor1_Model1\n\n\nObtain the final model (fit on the entire training set):\n\nfinal_fit |&gt;\n  extract_fit_parsnip() |&gt;\n  tidy()\n\n# A tibble: 28 × 5\n   term                                estimate std.error statistic  p.value\n   &lt;chr&gt;                                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                         22502.       1533.  14.7     4.55e-35\n 2 Humidity                            -2138.       1434.  -1.49    1.37e- 1\n 3 Seasons_Spring                      -1984.        238.  -8.33    6.67e-15\n 4 Seasons_Summer                       8057.        921.   8.75    4.20e-16\n 5 Seasons_Winter                      -4114.        994.  -4.14    4.88e- 5\n 6 Holiday_No.Holiday                    894.        191.   4.69    4.61e- 6\n 7 ww_weekend                          -1098.        169.  -6.50    4.66e-10\n 8 Seasons_Spring_x_Holiday_No.Holiday    -2.41      256.  -0.00942 9.92e- 1\n 9 Seasons_Summer_x_Holiday_No.Holiday  -126.        239.  -0.529   5.97e- 1\n10 Seasons_Winter_x_Holiday_No.Holiday  -314.        188.  -1.67    9.63e- 2\n# ℹ 18 more rows\n\n\n\n\nMLR Model (No Interaction)\nApply the model with no interactions to the test set:\n\nMLR_final_fit &lt;- MLR_wkf1 |&gt;\n  last_fit(bike_split, metrics = metric_set(mae, rmse))\n\nMLR_final_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard       3039. Preprocessor1_Model1\n2 rmse    standard       3980. Preprocessor1_Model1\n\n\nWe will compare this with the LASSO output!\n\n\nTuned Least Absolute Shrinkage and Selection Operator (LASSO)\nCreate a model instance with tune. Setting mixture = 1 turns this into a LASSO model, rather than an elastic net model. penalty = tune() tells tidymodels to use use a resampling method to choose this parameter.\n\nLASSO_spec &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt;\n  set_engine(\"glmnet\")\n\nGet the workflows:\n\nLASSO_wkf &lt;- workflow() |&gt;\n  add_recipe(bike_recipe1) |&gt;\n  add_model(LASSO_spec)\n\nFit the model:\n\nLASSO_grid &lt;- LASSO_wkf |&gt;\n  tune_grid(resamples = bike_10_fold,\n            grid = grid_regular(penalty(), levels = 200))\n\nCompute the metrics:\n\nLASSO_grid |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\")\n\n# A tibble: 200 × 7\n    penalty .metric .estimator  mean     n std_err .config               \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n 1 1   e-10 rmse    standard   4195.    10    160. Preprocessor1_Model001\n 2 1.12e-10 rmse    standard   4195.    10    160. Preprocessor1_Model002\n 3 1.26e-10 rmse    standard   4195.    10    160. Preprocessor1_Model003\n 4 1.41e-10 rmse    standard   4195.    10    160. Preprocessor1_Model004\n 5 1.59e-10 rmse    standard   4195.    10    160. Preprocessor1_Model005\n 6 1.78e-10 rmse    standard   4195.    10    160. Preprocessor1_Model006\n 7 2.00e-10 rmse    standard   4195.    10    160. Preprocessor1_Model007\n 8 2.25e-10 rmse    standard   4195.    10    160. Preprocessor1_Model008\n 9 2.52e-10 rmse    standard   4195.    10    160. Preprocessor1_Model009\n10 2.83e-10 rmse    standard   4195.    10    160. Preprocessor1_Model010\n# ℹ 190 more rows\n\n\nPlot to see:\n\nLASSO_grid |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(aes(penalty, mean, color = .metric)) +\n  geom_line()\n\n\n\n\n\n\n\n\nGet the tuning parameter corresponding to the best RMSE value and determine the coefficients of the model:\n\nlowest_rmse &lt;- LASSO_grid |&gt;\n  select_best(metric = \"rmse\")\nlowest_rmse\n\n# A tibble: 1 × 2\n       penalty .config               \n         &lt;dbl&gt; &lt;chr&gt;                 \n1 0.0000000001 Preprocessor1_Model001\n\n\nFit the “best” LASSO on the entire training set:\n\nLASSO_wkf |&gt;\n  finalize_workflow(lowest_rmse)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1e-10\n  mixture = 1\n\nComputational engine: glmnet \n\nLASSO_final &lt;- LASSO_wkf |&gt;\n  finalize_workflow(lowest_rmse) |&gt;\n  fit(bike_train)\ntidy(LASSO_final)\n\n# A tibble: 14 × 3\n   term                  estimate      penalty\n   &lt;chr&gt;                    &lt;dbl&gt;        &lt;dbl&gt;\n 1 (Intercept)             17446. 0.0000000001\n 2 Temperature               389. 0.0000000001\n 3 Humidity                 -887. 0.0000000001\n 4 Wind_speed               -522. 0.0000000001\n 5 Visibility                  0  0.0000000001\n 6 Dew_point_temperature    3752. 0.0000000001\n 7 Solar_Radiation          4065. 0.0000000001\n 8 Rainfall                -1841. 0.0000000001\n 9 Snowfall                 -336. 0.0000000001\n10 Seasons_Spring          -2505. 0.0000000001\n11 Seasons_Summer          -1607. 0.0000000001\n12 Seasons_Winter          -3653. 0.0000000001\n13 Holiday_No.Holiday        820. 0.0000000001\n14 WW_weekend              -1060. 0.0000000001\n\n\nApply the LASSO model to the test set:\n\nLASSO_final_fit &lt;- LASSO_wkf |&gt;\n  finalize_workflow(lowest_rmse) |&gt;\n  last_fit(bike_split, metrics = metric_set(mae, rmse))\n\nLASSO_final_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard       3063. Preprocessor1_Model1\n2 rmse    standard       3999. Preprocessor1_Model1\n\n\n\n\nTuned Regression Tree Model\nSet up the model type and engine:\n\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 20,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\nCreate workflows:\n\ntree_wkf &lt;- workflow() |&gt;\n  add_recipe(bike_recipe1) |&gt;\n  add_model(tree_mod)\n\ntemp &lt;- tree_wkf |&gt; \n  tune_grid(resamples = bike_10_fold)\ntemp |&gt; \n  collect_metrics()\n\n# A tibble: 20 × 8\n   cost_complexity tree_depth .metric .estimator     mean     n  std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;  \n 1        5.04e- 8         14 rmse    standard   4133.       10 186.     Prepro…\n 2        5.04e- 8         14 rsq     standard      0.823    10   0.0146 Prepro…\n 3        1.21e- 4         12 rmse    standard   4133.       10 186.     Prepro…\n 4        1.21e- 4         12 rsq     standard      0.823    10   0.0146 Prepro…\n 5        3.20e- 4         12 rmse    standard   4134.       10 185.     Prepro…\n 6        3.20e- 4         12 rsq     standard      0.823    10   0.0145 Prepro…\n 7        7.05e-10          3 rmse    standard   4764.       10 184.     Prepro…\n 8        7.05e-10          3 rsq     standard      0.771    10   0.0217 Prepro…\n 9        4.03e- 8          8 rmse    standard   4128.       10 184.     Prepro…\n10        4.03e- 8          8 rsq     standard      0.823    10   0.0143 Prepro…\n11        8.94e- 3          7 rmse    standard   4386.       10 206.     Prepro…\n12        8.94e- 3          7 rsq     standard      0.803    10   0.0190 Prepro…\n13        9.63e- 6          5 rmse    standard   4472.       10 177.     Prepro…\n14        9.63e- 6          5 rsq     standard      0.797    10   0.0161 Prepro…\n15        4.17e- 7          2 rmse    standard   4825.       10 168.     Prepro…\n16        4.17e- 7          2 rsq     standard      0.772    10   0.0214 Prepro…\n17        1.41e- 9          4 rmse    standard   4381.       10 225.     Prepro…\n18        1.41e- 9          4 rsq     standard      0.806    10   0.0207 Prepro…\n19        2.08e- 2         10 rmse    standard   4684.       10 172.     Prepro…\n20        2.08e- 2         10 rsq     standard      0.776    10   0.0197 Prepro…\n\n\nFit to cross validation (CV) folds:\n\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = c(10, 5))\n\ntree_fits &lt;- tree_wkf |&gt; \n  tune_grid(resamples = bike_10_fold,\n            grid = tree_grid)\n\ntree_fits |&gt;\n  collect_metrics()\n\n# A tibble: 100 × 8\n   cost_complexity tree_depth .metric .estimator     mean     n  std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001          1 rmse    standard   6613.       10 264.     Prepro…\n 2    0.0000000001          1 rsq     standard      0.559    10   0.0279 Prepro…\n 3    0.000000001           1 rmse    standard   6613.       10 264.     Prepro…\n 4    0.000000001           1 rsq     standard      0.559    10   0.0279 Prepro…\n 5    0.00000001            1 rmse    standard   6613.       10 264.     Prepro…\n 6    0.00000001            1 rsq     standard      0.559    10   0.0279 Prepro…\n 7    0.0000001             1 rmse    standard   6613.       10 264.     Prepro…\n 8    0.0000001             1 rsq     standard      0.559    10   0.0279 Prepro…\n 9    0.000001              1 rmse    standard   6613.       10 264.     Prepro…\n10    0.000001              1 rsq     standard      0.559    10   0.0279 Prepro…\n# ℹ 90 more rows\n\ntree_fits %&gt;%\n  collect_metrics() %&gt;%\n  mutate(tree_depth = factor(tree_depth)) %&gt;%\n  ggplot(aes(cost_complexity, mean, color = tree_depth)) +\n  geom_line(linewidth = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\", begin = .9, end = 0)\n\n\n\n\n\n\n\n\nCheck metric, sort by RMSE:\n\ntree_fits |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric .estimator  mean     n std_err .config    \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n 1    0.0000000001          8 rmse    standard   4128.    10    184. Preprocess…\n 2    0.000000001           8 rmse    standard   4128.    10    184. Preprocess…\n 3    0.00000001            8 rmse    standard   4128.    10    184. Preprocess…\n 4    0.0000001             8 rmse    standard   4128.    10    184. Preprocess…\n 5    0.000001              8 rmse    standard   4128.    10    184. Preprocess…\n 6    0.00001               8 rmse    standard   4128.    10    184. Preprocess…\n 7    0.0001                8 rmse    standard   4128.    10    184. Preprocess…\n 8    0.0000000001         11 rmse    standard   4133.    10    186. Preprocess…\n 9    0.000000001          11 rmse    standard   4133.    10    186. Preprocess…\n10    0.00000001           11 rmse    standard   4133.    10    186. Preprocess…\n# ℹ 40 more rows\n\n\nGet the best tuning parameter:\n\ntree_best_params &lt;- select_best(tree_fits, metric = 'rmse')\n\nRefit on the test set using this tuning parameter:\n\ntree_final_wkf &lt;- tree_wkf |&gt;\n  finalize_workflow(tree_best_params)\n\ntree_final_fit &lt;- tree_final_wkf |&gt;\n  last_fit(bike_split, metrics = metric_set(mae, rmse))\n\ntree_final_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard       2455. Preprocessor1_Model1\n2 rmse    standard       3182. Preprocessor1_Model1\n\n\nExtract the final model and plot the final fit:\n\ntree_final_model &lt;- extract_workflow(tree_final_fit)\n\ntree_final_model %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot::rpart.plot(roundint = FALSE)\n\n\n\n\n\n\n\n\n\n\nTuned Bootstrap Aggregated (Bagged) Model\nSet up the model type and engine:\n\nbag_spec &lt;- bag_tree(tree_depth = 5, min_n = 10, cost_complexity = tune()) |&gt;\n set_engine(\"rpart\") |&gt;\n set_mode(\"regression\")\n\nCreate workflows:\n\nbag_wkf &lt;- workflow() |&gt;\n add_recipe(bike_recipe1) |&gt;\n add_model(bag_spec)\n\nFit to cross validation (CV) folds:\n\nbag_fit &lt;- bag_wkf |&gt;\n tune_grid(resamples = bike_10_fold,\n grid = grid_regular(cost_complexity(),\n levels = 15),\n metrics = metric_set(mae, rmse))\n\nCheck metric, sort by RMSE:\n\nbag_fit |&gt;\n collect_metrics() |&gt;\n filter(.metric == \"rmse\") |&gt;\n arrange(mean)\n\n# A tibble: 15 × 7\n   cost_complexity .metric .estimator  mean     n std_err .config              \n             &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1        4.39e-10 rmse    standard   3250.    10    213. Preprocessor1_Model02\n 2        1   e-10 rmse    standard   3280.    10    229. Preprocessor1_Model01\n 3        7.20e- 7 rmse    standard   3311.    10    197. Preprocessor1_Model07\n 4        3.16e- 6 rmse    standard   3329.    10    229. Preprocessor1_Model08\n 5        8.48e- 9 rmse    standard   3336.    10    221. Preprocessor1_Model04\n 6        6.11e- 5 rmse    standard   3350.    10    202. Preprocessor1_Model10\n 7        1.64e- 7 rmse    standard   3357.    10    214. Preprocessor1_Model06\n 8        1.93e- 9 rmse    standard   3368.    10    187. Preprocessor1_Model03\n 9        1.18e- 3 rmse    standard   3380.    10    216. Preprocessor1_Model12\n10        2.68e- 4 rmse    standard   3388.    10    200. Preprocessor1_Model11\n11        5.18e- 3 rmse    standard   3393.    10    182. Preprocessor1_Model13\n12        1.39e- 5 rmse    standard   3449.    10    201. Preprocessor1_Model09\n13        3.73e- 8 rmse    standard   3453.    10    233. Preprocessor1_Model05\n14        2.28e- 2 rmse    standard   3992.    10    182. Preprocessor1_Model14\n15        1   e- 1 rmse    standard   5064.    10    203. Preprocessor1_Model15\n\n\nGet the best tuning parameter:\n\nbag_best_params &lt;- select_best(bag_fit, metric = 'rmse')\n\nRefit on the entire training set using this tuning parameter:\n\nbag_final_wkf &lt;- bag_wkf |&gt;\n finalize_workflow(bag_best_params)\nbag_final_fit &lt;- bag_final_wkf |&gt;\n last_fit(bike_split, metrics = metric_set(mae, rmse))\n\nInvestigate the bagged tree model. Refit to the entire data set:\n\nbag_full_fit &lt;- bag_final_wkf |&gt;\n  fit(seoul_bike_data)\n\nExtract the final model:\n\nbag_final_model &lt;- extract_fit_engine(bag_full_fit)\nattributes(bag_final_model)\n\n$names\n[1] \"model_df\"   \"control\"    \"cost\"       \"imp\"        \"base_model\"\n[6] \"blueprint\" \n\n$class\n[1] \"bagger\"         \"hardhat_model\"  \"hardhat_scalar\"\n\n\nProduce a variable importance plot to examine the final model:\n\nbag_final_model$imp |&gt;\n mutate(term = factor(term, levels = term)) |&gt;\n ggplot(aes(x = term, y = value)) +\n geom_bar(stat =\"identity\") +\n coord_flip()\n\n\n\n\n\n\n\n\n\n\nTuned Random Forest Model\nUse the same recipe, but fit with a random forest model:\n\n#With randomForest package\n#rf_spec &lt;- rand_forest(mtry = tune()) |&gt;\n# set_engine(\"randomForest\") |&gt;\n# set_mode(\"regression\")\n\n#With ranger package\nrf_spec &lt;- rand_forest(mtry = tune()) |&gt;\n set_engine(\"ranger\", importance = \"impurity\") |&gt;\n set_mode(\"regression\")\n\nCreate the workflows:\n\nrf_wkf &lt;- workflow() |&gt;\n add_recipe(bike_recipe1) |&gt;\n add_model(rf_spec)\n\nFit to the cross validation (CV) folds:\n\nrf_fit &lt;- rf_wkf |&gt;\n tune_grid(resamples = bike_10_fold,\n grid = 7,\n metrics = metric_set(mae, rmse))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nSort by RMSE:\n\nrf_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 7 × 7\n   mtry .metric .estimator  mean     n std_err .config             \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1     7 rmse    standard   3007.    10    182. Preprocessor1_Model5\n2    10 rmse    standard   3020.    10    184. Preprocessor1_Model6\n3     6 rmse    standard   3031.    10    179. Preprocessor1_Model7\n4     8 rmse    standard   3033.    10    188. Preprocessor1_Model1\n5    13 rmse    standard   3047.    10    188. Preprocessor1_Model2\n6     4 rmse    standard   3095.    10    169. Preprocessor1_Model3\n7     2 rmse    standard   3451.    10    165. Preprocessor1_Model4\n\n\nObtain the best tuning parameter:\n\n#Obtain the best tuning parameter\nrf_best_params &lt;- select_best(rf_fit, metric = 'rmse')\n\nApply to the test set using this tuning parameter:\n\n#Refit on the entire training set using this tuning parameter\nrf_final_wkf &lt;- rf_wkf |&gt;\n  finalize_workflow(rf_best_params)\n\nrf_final_fit &lt;- rf_final_wkf |&gt;\n  last_fit(bike_split, metrics = metric_set(mae, rmse))\n\nInvestigate the random forest model. Refit to the entire data set:\n\n#Investigate the random forest model\n#Refit to the entire data set\nrf_full_fit &lt;- rf_final_wkf |&gt;\n  fit(seoul_bike_data)\nrf_full_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~7L,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      353 \nNumber of independent variables:  13 \nMtry:                             7 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       7748586 \nR squared (OOB):                  0.9215311 \n\n\nExtract the final model:\n\nrf_final_model &lt;- extract_fit_engine(rf_full_fit)\nattributes(rf_final_model)\n\n$names\n [1] \"predictions\"               \"num.trees\"                \n [3] \"num.independent.variables\" \"mtry\"                     \n [5] \"min.node.size\"             \"variable.importance\"      \n [7] \"prediction.error\"          \"forest\"                   \n [9] \"splitrule\"                 \"treetype\"                 \n[11] \"r.squared\"                 \"call\"                     \n[13] \"importance.mode\"           \"num.samples\"              \n[15] \"replace\"                   \"max.depth\"                \n\n$class\n[1] \"ranger\"\n\n\nProduce a variable importance plot to examine the final model:\n\n#With randomForest package\n#imp &lt;- cbind.data.frame(Feature=rownames(rf_final_model$importance),rf_final_model$importance)\n#ggplot(imp, aes(x=reorder(Feature, -IncNodePurity), y=IncNodePurity)) +\n#  geom_bar(stat = 'identity') + \n#  xlab('term') +\n#  ylab('value') +\n#  coord_flip()\n\n#With ranger package\nimp &lt;- enframe(rf_final_model$variable.importance,\n        name = \"variable\",\n        value = \"importance\")\nggplot(imp, aes(x = reorder(variable, -importance), y = importance)) +\n  geom_bar(stat = 'identity') + \n  xlab('term') +\n  ylab('value') +\n  coord_flip()\n\n\n\n\n\n\n\n\nCompare models on the test set:\n\nMLR_final_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard       3039. Preprocessor1_Model1\n2 rmse    standard       3980. Preprocessor1_Model1\n\nLASSO_final_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard       3063. Preprocessor1_Model1\n2 rmse    standard       3999. Preprocessor1_Model1\n\ntree_final_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard       2455. Preprocessor1_Model1\n2 rmse    standard       3182. Preprocessor1_Model1\n\nbag_final_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard       2416. Preprocessor1_Model1\n2 rmse    standard       3029. Preprocessor1_Model1\n\nrf_final_fit |&gt; collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard       2250. Preprocessor1_Model1\n2 rmse    standard       2809. Preprocessor1_Model1\n\n\nRandom forest has the best fit with the smallest values for MAE and RMSE."
  }
]